# Рекомендации по оптимизации производительности

## Текущая ситуация
- **Время выполнения**: ~6 часов для 6977 вопросов
- **Основные узкие места**: 
  1. Построение FAISS индекса (~2.4 часа)
  2. Обработка вопросов в submit.py (~3-4 часа)

## Быстрые оптимизации (без изменения кода)

### 1. Уменьшить k_retrieve
В `configs/base.yaml` измените:
```yaml
retrieval:
  k_retrieve: 30  # было 50, уменьшаем до 30-40
```
**Эффект**: Ускорит реранкинг на ~40% (меньше кандидатов для обработки)

### 2. Использовать GPU (если доступно)
В `configs/models.yaml` измените:
```yaml
embeddings:
  device: "cuda"  # было "cpu"
  
reranker:
  device: "cuda"  # было "cpu"
  batch_size: 32  # можно увеличить до 32-64 на GPU
```
**Эффект**: Ускорение в 5-10 раз для эмбеддингов и реранкера

### 3. Увеличить batch_size для реранкера
В `configs/models.yaml`:
```yaml
reranker:
  batch_size: 32  # было 16, можно до 64 на GPU
```
**Эффект**: Меньше вызовов модели, выше throughput

## Ожидаемое время после оптимизаций

| Конфигурация | Время submit.py | Время build_index |
|--------------|----------------|-------------------|
| CPU, k_retrieve=50 | ~4 часа | ~2.4 часа |
| CPU, k_retrieve=30 | ~2.5 часа | ~2.4 часа |
| GPU, k_retrieve=30 | ~20-30 мин | ~15-20 мин |

## Дополнительные оптимизации (требуют изменений кода)

### 1. Батчинг на уровне submit.py
Обрабатывать несколько вопросов параллельно (уже добавлен прогресс-бар)

### 2. Кэширование эмбеддингов
Кэшировать эмбеддинги для одинаковых запросов

### 3. Использовать более быстрый реранкер
```yaml
reranker:
  model_name: "cross-encoder/ms-marco-MiniLM-L-6-v2"  # быстрее, но менее точный
```

### 4. Отключить реранкер для тестирования
```yaml
retrieval:
  use_reranker: false  # для быстрого тестирования
```

## Проверка наличия GPU

```bash
python -c "import torch; print('CUDA available:', torch.cuda.is_available())"
```

Если выводит `True`, можно использовать `device: "cuda"`.

