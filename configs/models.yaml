# Model configurations for embeddings, reranking, and tokenization

embeddings:
  model_name: "intfloat/multilingual-e5-base"
  # alternatives: 
  # - "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
  # - "intfloat/multilingual-e5-large"
  device: "auto"          # must resolve to CUDA; raises if kernels absent
  batch_size: 128         # optimized batch size for GPU throughput
  normalize_embeddings: true
  use_fp16: true          # enable half precision on GPU

reranker:
  model_name: "cross-encoder/ms-marco-MiniLM-L-12-v2"
  # alternatives:
  # - "cross-encoder/ms-marco-MiniLM-L-6-v2" (faster, less accurate)
  # - "BAAI/bge-reranker-base" (multilingual)
  device: "auto"
  batch_size: 64          # optimized batch size for GPU
  use_fp16: true

tokenizer:
  name: "o200k_base"
  # alternatives:
  # - "word" (simple word-based)
  # - path to huggingface tokenizer

faiss:
  index_type: "Flat"  # "Flat", "IVFFlat", "IVFPQ"
  # for large datasets, consider IVFFlat or IVFPQ
  nprobe: 10  # for IVF indices
  use_gpu: true           # FAISS index is cloned to GPU
  gpu_device: 0
  use_float16: true

bm25:
  backend: "bm25s"  # "bm25s" or "pyserini"
  k1: 1.5
  b: 0.75

